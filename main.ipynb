{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from skimage.io import imsave\n",
    "from torchvision.utils import save_image\n",
    "from utils import compute_gt_gradient, make_canvas_mask, numpy2tensor, laplacian_filter_tensor, \\\n",
    "                  MeanShift, Vgg16, gram_matrix\n",
    "import argparse\n",
    "import pdb\n",
    "import os\n",
    "import imageio.v2 as iio\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "########### First Pass ###########\n",
    "###################################\n",
    "\n",
    "# Inputs\n",
    "source_file = \"data/rsz_ad_jotform.png\"\n",
    "mask_file = \"data/rsz_ad_mask_jotform.png\"\n",
    "target_file = \"data/avatar_700_bg.png\"\n",
    "\n",
    "# Outputs\n",
    "output_dir = \"results/1/\"\n",
    "\n",
    "# Hyperparameter Inputs\n",
    "gpu_id = 1\n",
    "num_steps = 500\n",
    "ts = 1024\n",
    "ss = 50\n",
    "\n",
    "# blending operation\n",
    "x_start = 512\n",
    "y_start = 512\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "num_steps = 1000\n",
    "\n",
    "save_video = False\n",
    "\n",
    "# Default weights for loss functions in the first pass\n",
    "grad_weight = 1e4; style_weight = 1e4; content_weight = 1; tv_weight = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run [0]:\n",
      "grad : 278022.000000, style : 15176670.000000, content: 73046.148438, tv: 13.910302\n",
      "\n",
      "run [1]:\n",
      "grad : 278020.531250, style : 15176670.000000, content: 73046.109375, tv: 13.910302\n",
      "\n",
      "run [2]:\n",
      "grad : 170658.015625, style : 15157843.000000, content: 58511.398438, tv: 14.031243\n",
      "\n",
      "run [3]:\n",
      "grad : 86077.148438, style : 15151732.000000, content: 51578.355469, tv: 14.004866\n",
      "\n",
      "run [4]:\n",
      "grad : 65655.554688, style : 15129552.000000, content: 43953.585938, tv: 14.029720\n",
      "\n",
      "run [5]:\n",
      "grad : 82639.726562, style : 15046895.000000, content: 41416.332031, tv: 14.091745\n",
      "\n",
      "run [6]:\n",
      "grad : 225467.171875, style : 14715626.000000, content: 87867.062500, tv: 14.240730\n",
      "\n",
      "run [7]:\n",
      "grad : 1958781.000000, style : 12265021.000000, content: 869668.125000, tv: 15.011994\n",
      "\n",
      "run [8]:\n",
      "grad : 614841.437500, style : 13593513.000000, content: 278864.593750, tv: 14.537506\n",
      "\n",
      "run [9]:\n",
      "grad : 650040.000000, style : 12819596.000000, content: 326062.843750, tv: 14.599748\n",
      "\n",
      "run [10]:\n",
      "grad : 7779883.000000, style : 71872600.000000, content: 3188041.750000, tv: 16.312578\n",
      "\n",
      "run [11]:\n",
      "grad : 403049.343750, style : 12613981.000000, content: 226387.906250, tv: 14.491790\n",
      "\n",
      "run [12]:\n",
      "grad : 384986.906250, style : 12260916.000000, content: 231248.171875, tv: 14.491760\n",
      "\n",
      "run [13]:\n",
      "grad : 3676239.000000, style : 29160208.000000, content: 1274126.875000, tv: 15.502887\n",
      "\n",
      "run [14]:\n",
      "grad : 348737.937500, style : 11472769.000000, content: 259980.281250, tv: 14.486903\n",
      "\n",
      "run [15]:\n",
      "grad : 333654.406250, style : 10894510.000000, content: 264268.531250, tv: 14.484260\n",
      "\n",
      "run [16]:\n",
      "grad : 998066.000000, style : 14731097.000000, content: 571861.000000, tv: 14.841836\n",
      "\n",
      "run [17]:\n",
      "grad : 291156.156250, style : 10046191.000000, content: 258112.703125, tv: 14.459568\n",
      "\n",
      "run [18]:\n",
      "grad : 265301.375000, style : 9639308.000000, content: 249429.375000, tv: 14.443010\n",
      "\n",
      "run [19]:\n",
      "grad : 291671.531250, style : 9483660.000000, content: 265966.062500, tv: 14.453158\n",
      "\n",
      "run [20]:\n",
      "grad : 194190.390625, style : 9047898.000000, content: 223664.640625, tv: 14.380082\n",
      "\n",
      "run [21]:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m         run[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m--> 124\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# clamp the pixels range into 0 ~ 255\u001b[39;00m\n\u001b[1;32m    127\u001b[0m input_img\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torch/optim/lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 438\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    439\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    440\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dib_jotform/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 114\u001b[0m, in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(run))\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad : \u001b[39m\u001b[38;5;132;01m{:4f}\u001b[39;00m\u001b[38;5;124m, style : \u001b[39m\u001b[38;5;132;01m{:4f}\u001b[39;00m\u001b[38;5;124m, content: \u001b[39m\u001b[38;5;132;01m{:4f}\u001b[39;00m\u001b[38;5;124m, tv: \u001b[39m\u001b[38;5;132;01m{:4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\\\n\u001b[0;32m--> 114\u001b[0m                   \u001b[43mgrad_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \\\n\u001b[1;32m    115\u001b[0m                   style_loss\u001b[38;5;241m.\u001b[39mitem(), \\\n\u001b[1;32m    116\u001b[0m                   content_loss\u001b[38;5;241m.\u001b[39mitem(), \\\n\u001b[1;32m    117\u001b[0m                   tv_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    118\u001b[0m                   ))\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    121\u001b[0m run[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Images\n",
    "source_img = np.array(Image.open(source_file).convert('RGB').resize((ss, ss)))\n",
    "target_img = np.array(Image.open(target_file).convert('RGB').resize((ts, ts)))\n",
    "mask_img = np.array(Image.open(mask_file).convert('L').resize((ss, ss)))\n",
    "mask_img[mask_img>0] = 1\n",
    "\n",
    "# Make Canvas Mask\n",
    "canvas_mask = make_canvas_mask(x_start, y_start, target_img, mask_img)\n",
    "canvas_mask = numpy2tensor(canvas_mask, device)\n",
    "canvas_mask = canvas_mask.squeeze(0).repeat(3,1).view(3,ts,ts).unsqueeze(0)\n",
    "\n",
    "# Compute Ground-Truth Gradients\n",
    "gt_gradient = compute_gt_gradient(x_start, y_start, source_img, target_img, mask_img, device)\n",
    "\n",
    "# Convert Numpy Images Into Tensors\n",
    "#source_img = torch.from_numpy(source_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(gpu_id)\n",
    "#target_img = torch.from_numpy(target_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(gpu_id)\n",
    "#input_img = torch.randn(target_img.shape).to(gpu_id)\n",
    "\n",
    "source_img = torch.from_numpy(source_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(device)\n",
    "target_img = torch.from_numpy(target_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(device)\n",
    "input_img = torch.randn(target_img.shape).to(device)\n",
    "\n",
    "mask_img = numpy2tensor(mask_img, device)\n",
    "mask_img = mask_img.squeeze(0).repeat(3,1).view(3,ss,ss).unsqueeze(0)\n",
    "\n",
    "# Define LBFGS optimizer\n",
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "# Define Loss Functions\n",
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "# Import VGG network for computing style and content loss\n",
    "mean_shift = MeanShift(device)\n",
    "#vgg = Vgg16().to(gpu_id)\n",
    "vgg = Vgg16().to(device)\n",
    "\n",
    "# Save reconstruction process in a video\n",
    "if save_video:\n",
    "    recon_process_video = iio.get_writer(os.path.join(output_dir, 'recon_process.mp4'), format='FFMPEG', mode='I', fps=400)\n",
    "\n",
    "run = [0]\n",
    "while run[0] <= num_steps:\n",
    "    \n",
    "    def closure():\n",
    "        # Composite Foreground and Background to Make Blended Image\n",
    "\n",
    "        #blend_img = torch.zeros(target_img.shape).to(gpu_id)\n",
    "        blend_img = torch.zeros(target_img.shape).to(device)\n",
    "        blend_img = input_img*canvas_mask + target_img*(canvas_mask-1)*(-1) \n",
    "        \n",
    "        # Compute Laplacian Gradient of Blended Image\n",
    "        pred_gradient = laplacian_filter_tensor(blend_img, device)\n",
    "        \n",
    "        # Compute Gradient Loss\n",
    "        grad_loss = 0\n",
    "        for c in range(len(pred_gradient)):\n",
    "            grad_loss += mse(pred_gradient[c], gt_gradient[c])\n",
    "        grad_loss /= len(pred_gradient)\n",
    "        grad_loss *= grad_weight\n",
    "        \n",
    "        # Compute Style Loss\n",
    "        target_features_style = vgg(mean_shift(target_img))\n",
    "        target_gram_style = [gram_matrix(y) for y in target_features_style]\n",
    "        \n",
    "        blend_features_style = vgg(mean_shift(input_img))\n",
    "        blend_gram_style = [gram_matrix(y) for y in blend_features_style]\n",
    "        \n",
    "        style_loss = 0\n",
    "        for layer in range(len(blend_gram_style)):\n",
    "            style_loss += mse(blend_gram_style[layer], target_gram_style[layer])\n",
    "        style_loss /= len(blend_gram_style)  \n",
    "        style_loss *= style_weight           \n",
    "\n",
    "        \n",
    "        # Compute Content Loss\n",
    "        blend_obj = blend_img[:,:,int(x_start-source_img.shape[2]*0.5):int(x_start+source_img.shape[2]*0.5), int(y_start-source_img.shape[3]*0.5):int(y_start+source_img.shape[3]*0.5)]\n",
    "        source_object_features = vgg(mean_shift(source_img*mask_img))\n",
    "        blend_object_features = vgg(mean_shift(blend_obj*mask_img))\n",
    "        content_loss = content_weight * mse(blend_object_features.relu2_2, source_object_features.relu2_2)\n",
    "        content_loss *= content_weight\n",
    "        \n",
    "        # Compute TV Reg Loss\n",
    "        tv_loss = torch.sum(torch.abs(blend_img[:, :, :, :-1] - blend_img[:, :, :, 1:])) + \\\n",
    "                   torch.sum(torch.abs(blend_img[:, :, :-1, :] - blend_img[:, :, 1:, :]))\n",
    "        tv_loss *= tv_weight\n",
    "        \n",
    "        # Compute Total Loss and Update Image\n",
    "        loss = grad_loss + style_loss + content_loss + tv_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Write to output to a reconstruction video \n",
    "        if save_video:\n",
    "            foreground = input_img*canvas_mask\n",
    "            foreground = (foreground - foreground.min()) / (foreground.max() - foreground.min())\n",
    "            background = target_img*(canvas_mask-1)*(-1)\n",
    "            background = background / 255.0\n",
    "            final_blend_img =  + foreground + background\n",
    "            if run[0] < 200:\n",
    "                # more frames for early optimization by repeatedly appending the frames\n",
    "                for _ in range(10):\n",
    "                    recon_process_video.append_data(final_blend_img[0].transpose(0,2).transpose(0,1).cpu().data.numpy())\n",
    "            else:\n",
    "                recon_process_video.append_data(final_blend_img[0].transpose(0,2).transpose(0,1).cpu().data.numpy())\n",
    "        \n",
    "        # Print Loss\n",
    "        if run[0] % 1 == 0:\n",
    "            print(\"run {}:\".format(run))\n",
    "            print('grad : {:4f}, style : {:4f}, content: {:4f}, tv: {:4f}'.format(\\\n",
    "                          grad_loss.item(), \\\n",
    "                          style_loss.item(), \\\n",
    "                          content_loss.item(), \\\n",
    "                          tv_loss.item()\n",
    "                          ))\n",
    "            print()\n",
    "        \n",
    "        run[0] += 1\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "\n",
    "# clamp the pixels range into 0 ~ 255\n",
    "input_img.data.clamp_(0, 255)\n",
    "\n",
    "# Make the Final Blended Image\n",
    "#blend_img = torch.zeros(target_img.shape).to(gpu_id)\n",
    "blend_img = torch.zeros(target_img.shape).to(device)\n",
    "blend_img = input_img*canvas_mask + target_img*(canvas_mask-1)*(-1) \n",
    "blend_img_np = blend_img.transpose(1,3).transpose(1,2).cpu().data.numpy()[0]\n",
    "\n",
    "# Save image from the first pass\n",
    "first_pass_img_file = os.path.join(output_dir, 'first_pass.png')\n",
    "imsave(first_pass_img_file, blend_img_np.astype(np.uint8))\n",
    "\n",
    "###################################\n",
    "########### Second Pass ###########\n",
    "###################################\n",
    "\n",
    "# Default weights for loss functions in the second pass\n",
    "style_weight = 1e7; content_weight = 1; tv_weight = 1e-6\n",
    "#ss = 512; ts = 512\n",
    "ss = ts;\n",
    "num_steps = num_steps\n",
    "\n",
    "first_pass_img = np.array(Image.open(first_pass_img_file).convert('RGB').resize((ss, ss)))\n",
    "target_img = np.array(Image.open(target_file).convert('RGB').resize((ts, ts)))\n",
    "\n",
    "#first_pass_img = torch.from_numpy(first_pass_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(gpu_id)\n",
    "#target_img = torch.from_numpy(target_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(gpu_id)\n",
    "first_pass_img = torch.from_numpy(first_pass_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(device)\n",
    "target_img = torch.from_numpy(target_img).unsqueeze(0).transpose(1,3).transpose(2,3).float().to(device)\n",
    "\n",
    "first_pass_img = first_pass_img.contiguous()\n",
    "target_img = target_img.contiguous()\n",
    "\n",
    "# Define LBFGS optimizer\n",
    "def get_input_optimizer(first_pass_img):\n",
    "    optimizer = optim.LBFGS([first_pass_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "optimizer = get_input_optimizer(first_pass_img)\n",
    "\n",
    "print('Optimizing...')\n",
    "run = [0]\n",
    "while run[0] <= num_steps:\n",
    "    \n",
    "    def closure():\n",
    "        \n",
    "        # Compute Loss Loss    \n",
    "        target_features_style = vgg(mean_shift(target_img))\n",
    "        target_gram_style = [gram_matrix(y) for y in target_features_style]\n",
    "        blend_features_style = vgg(mean_shift(first_pass_img))\n",
    "        blend_gram_style = [gram_matrix(y) for y in blend_features_style]\n",
    "        style_loss = 0\n",
    "        for layer in range(len(blend_gram_style)):\n",
    "            style_loss += mse(blend_gram_style[layer], target_gram_style[layer])\n",
    "        style_loss /= len(blend_gram_style)  \n",
    "        style_loss *= style_weight        \n",
    "        \n",
    "        # Compute Content Loss\n",
    "        content_features = vgg(mean_shift(first_pass_img))\n",
    "        content_loss = content_weight * mse(blend_features_style.relu2_2, content_features.relu2_2)\n",
    "        \n",
    "        # Compute Total Loss and Update Image\n",
    "        loss = style_loss + content_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Write to output to a reconstruction video \n",
    "        if save_video:\n",
    "            foreground = first_pass_img*canvas_mask\n",
    "            foreground = (foreground - foreground.min()) / (foreground.max() - foreground.min())\n",
    "            background = target_img*(canvas_mask-1)*(-1)\n",
    "            background = background / 255.0\n",
    "            final_blend_img =  + foreground + background\n",
    "            recon_process_video.append_data(final_blend_img[0].transpose(0,2).transpose(0,1).cpu().data.numpy())\n",
    "        \n",
    "        # Print Loss\n",
    "        if run[0] % 1 == 0:\n",
    "            print(\"run {}:\".format(run))\n",
    "            print(' style : {:4f}, content: {:4f}'.format(\\\n",
    "                          style_loss.item(), \\\n",
    "                          content_loss.item()\n",
    "                          ))\n",
    "            print()\n",
    "        \n",
    "        run[0] += 1\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "\n",
    "# clamp the pixels range into 0 ~ 255\n",
    "first_pass_img.data.clamp_(0, 255)\n",
    "\n",
    "# Make the Final Blended Image\n",
    "input_img_np = first_pass_img.transpose(1,3).transpose(1,2).cpu().data.numpy()[0]\n",
    "\n",
    "# Save image from the second pass\n",
    "imsave(os.path.join(output_dir, 'second_pass.png'), input_img_np.astype(np.uint8))\n",
    "\n",
    "# Save recon process video\n",
    "if save_video:\n",
    "    recon_process_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dib_jotform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
